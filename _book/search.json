[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Statistics the Fun Way. Een notitieboek",
    "section": "",
    "text": "Preface\nThis Quarto book collects my personal notes, trials and exercises of the Bayesian Statistics the Fun Way: Understanding Statistics and Probability With Star Wars, LEGO, and Rubber Ducks by Will Kurt (kurt2019?).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-intro.html",
    "href": "00-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Frequentist statistics is founded on the idea that probability represents the frequency with which something happens.\n\n\nFrequentist statistics is a branch of statistics that interprets probability as the long-run frequency of events in repeated trials, focusing on parameter estimation, hypothesis testing, and confidence intervals without incorporating prior beliefs\nBayesian statistics on the other hand, is concerned with how probabilities represent how uncertain we are about a piece of information.\n\n\n\n\n\n\n\nBayesian statistics is a branch of statistics that interprets probability as a measure of belief or certainty, updating prior beliefs with new evidence using Bayes’ theorem to provide a posterior probability distribution for parameters of interest.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01-everyday-reasoning.html",
    "href": "01-everyday-reasoning.html",
    "title": "\n1  Bayesian Thinking and Everyday Reasoning\n",
    "section": "",
    "text": "1.1 Reasoning About Strange Experiences\nBayesian reasoning procedure:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian Thinking and Everyday Reasoning</span>"
    ]
  },
  {
    "objectID": "02-measuring-uncertainty.html",
    "href": "02-measuring-uncertainty.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "03-logic-uncertainty.html",
    "href": "03-logic-uncertainty.html",
    "title": "3  Logic of Uncertainty",
    "section": "",
    "text": "3.1 Combining Probabilities with AND",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Logic of Uncertainty</span>"
    ]
  },
  {
    "objectID": "04-binomial-distribution.html",
    "href": "04-binomial-distribution.html",
    "title": "4  Creating a Binomial Probability Distribution",
    "section": "",
    "text": "4.1 Structure of a Binomial Distribution\nA binomial distribution is used to calculate the probability of a certain number of successful outcomes, given a number of trials and the probability of the successful outcome. The “bi” in the term binomial refers to the two possible outcomes that we’re concerned with: an event happening and an event not happening. (If there are more than two outcomes, the distribution is called multinomial.)\nExamples for a binomial distribution are:\nCalculating the probability of flipping two heads in three coin tosses:\nFor the example of two heads in three coin tosses, we would write \\(B(2; 3, 1/2)\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating a Binomial Probability Distribution</span>"
    ]
  },
  {
    "objectID": "05-beta-distribution.html",
    "href": "05-beta-distribution.html",
    "title": "5  The Beta Distribution",
    "section": "",
    "text": "5.1 A Strange Scenario: Getting the Data\nIf you drop a quarter into a black box, it eject sometimes two quarter but sometimes it “eats” your quarter. So the question is: “What’s the probability of getting two quarters?”",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Beta Distribution</span>"
    ]
  },
  {
    "objectID": "01-everyday-reasoning.html#reasoning-about-strange-experiences",
    "href": "01-everyday-reasoning.html#reasoning-about-strange-experiences",
    "title": "\n1  Bayesian Thinking and Everyday Reasoning\n",
    "section": "",
    "text": "Note\n\n\n\nWill Kurt show with an UFO example how Bayesian thinking about beliefs and their updates when new data come is very natural and a common sense procedure. This is an interesting approach I haven’t thought before, because many Bayesian introductions are not only very complex but develops formulae that we are not using in the everyday world.\n\n\n\n\nObserve data\nBuild a hypothesis\nUpdate your beliefs based on new data\n\n\n1.1.1 Observing Data\n\\[P(\\text{bright light outside window}, \\text{saucer-shaped object in sky}) = \\text{very low}\\] You would read this equation as: “The probability of observing bright lights outside the window and a saucer-shaped object in the sky is very low.” In probability theory, we use a comma to separate events when we’re looking at the combined probability of multiple events.\n\n1.1.2 Holding Prior Beliefs and Conditioning Probabilities\n\\[\n\\begin{align*}\nP(\\text{bright light outside window},\\\\\n\\text{saucer-shaped object in sky} \\mid \\text{eperience on Earth}) = \\text{very low}\n\\end{align*}\n\\tag{1.1}\\] We would read this equation as: “The probability of observing bright lights and a saucer-shaped object in the sky, given our experience on Earth, is very low.”\nThe probability outcome is called a conditional probability (GLOSSARY) because we are conditioning the probability of one event occurring on the existence of something else.\nShorter variable names for events and conditions:\n\nD: all of our data\nX: prior belief\n\nLet \\(D = \\text{bright light outside window}, \\text{saucer-shaped object in sky}\\) and \\(X = \\text{experience on Earth}\\) then we can wrote Equation 1.1 as \\(P(D \\mid X) = \\text{very low})\\).\n\n1.1.3 Conditioning on Multiple Beliefs\n\\[\n\\begin{align*}\nP(\\text{bright light outside window},\\\\\n\\text{saucer-shaped object in sky} \\mid \\\\\n\\text{July 4th, eperience on Earth}) = \\text{low}\n\\end{align*}\n\\tag{1.2}\\] Taking both these experiences into account, our conditional probability changed from “very low” to “low.”\n\n1.1.4 Assuming Prior Beliefs in Practice\nIn order to explain what you saw, you need to form some kind of hypothesis—a model about how the world works that makes a prediction. All of our basic beliefs about the world are hypotheses.\n\nIf you believe the Earth rotates, you predict the sun will rise and set at certain times.\nIf you believe that your favorite baseball team is the best, you predict they will win more than the other teams.\nA scientist may hypothesize that a certain treatment will slow the growth of cancer.\nA quantitative analyst in finance may have a model of how the market will behave.\n\n\\[H_{1} = \\text{A UFO is in my backyard!}\\]\nBut what is this hypothesis predicting? We might ask, “If there was a UFO in your back yard, what would you expect to see?” And you might answer, “Bright lights and a saucer-shaped object.” Formally we write this as:\n\\[\nP(D \\mid H_{1}, X) &gt;&gt; P(D \\mid X)\n\\]\nThis equation says: “The probability of seeing bright lights and a saucer-shaped object in the sky, given my belief that this is a UFO and my prior experience, is much higher [indicated by the double greater-than sign &gt;&gt;] than just seeing bright lights and a saucer-shaped object in the sky without explanation.”\n\n1.1.5 Spotting Hypotheses in Everyday Speech\n\nSaying something is “surprising,” for example, might be the same as saying it has low-probability data based on our prior experiences.\nSaying something “makes sense” might indicate we have high-probability data based on our prior experiences.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian Thinking and Everyday Reasoning</span>"
    ]
  },
  {
    "objectID": "01-everyday-reasoning.html#gathering-more-evidence-and-updating-your-beliefs",
    "href": "01-everyday-reasoning.html#gathering-more-evidence-and-updating-your-beliefs",
    "title": "\n1  Bayesian Thinking and Everyday Reasoning\n",
    "section": "\n1.2 Gathering More Evidence and Updating Your Beliefs",
    "text": "1.2 Gathering More Evidence and Updating Your Beliefs\nTo collect more data, we need to make more observations. In our scenario, you look out your window: With new evidence, you realize it looks more like someone is shooting a movie nearby.\nBayesian analysis process\n\nYou started with your initial hypothesis: \\(H_{1} = \\text{A UFO is in my backyard!}\\).\nIn isolation, this hypothesis, given your experience, is extremely unlikely: \\(P(H_{1} \\mid X) = \\text{very, very low}\\)\n\nWith new data you are going to update your belief: \\(H_{2} = \\text{A film is being made}\\).\nIn isolation, the probability of this hypothesis is also intuitively very low: \\(P(H_{1} \\mid X) = \\text{very low}\\)\n\nYou updated your prior belief from “very, very low” to “very low”.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian Thinking and Everyday Reasoning</span>"
    ]
  },
  {
    "objectID": "01-everyday-reasoning.html#comparing-hypotheses",
    "href": "01-everyday-reasoning.html#comparing-hypotheses",
    "title": "\n1  Bayesian Thinking and Everyday Reasoning\n",
    "section": "\n1.3 Comparing Hypotheses",
    "text": "1.3 Comparing Hypotheses\nWith new data you have formed an alternate hypothesis. Let’s break this process down into Bayesian reasoning. Your first hypothesis, \\(H_{1}\\), gave you a way to explain your data and end your confusion, but with your additional observations \\(H_{1}\\) no longer explains the data well:\nYou started with\n\\[P(D \\mid H_{1}, X) = \\text{very, very low}\\] and updated our belief with\n\\[P(D_{updated} \\mid H_{2}, X) &gt;&gt; P(D \\mid H_{1}, X)\\] We say that one belief is more accurate than another because it provides a better explanation of the world we observe. Mathematically, we express this idea as the ratio of the two probabilities:\n\\[\\frac{P(D_{updated} \\mid H_{2}, X)}{P(D \\mid H_{1}, X)}\\]\nWhen this ratio is a large number, say 1,000, it means “\\(H_{2}\\) explains the data 1,000 times better than \\(H_{1}\\).”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian Thinking and Everyday Reasoning</span>"
    ]
  },
  {
    "objectID": "01-everyday-reasoning.html#data-informs-belief-belief-should-not-inform-data",
    "href": "01-everyday-reasoning.html#data-informs-belief-belief-should-not-inform-data",
    "title": "\n1  Bayesian Thinking and Everyday Reasoning\n",
    "section": "\n1.4 Data Informs Belief; Belief Should Not Inform Data",
    "text": "1.4 Data Informs Belief; Belief Should Not Inform Data\nOne final point worth stressing is that the only absolute in all these examples is your data. Your hypotheses change, and your experience in the world, \\(X\\), may be different from someone else’s, but the data, \\(D\\), is shared by all.\nCase 1 (used throughout this chapter):\n\\[P(D \\mid H, X) \\tag{1.3}\\] “How well do my beliefs explain what I observe?”\nCase 2 (used often in everyday thinking)\n\\[P(H \\mid D, X) \\tag{1.4}\\]\nIn the first case, we change our beliefs according to data we gather and observations we make about the world that describe it better. In the second case, we gather data to support our existing beliefs. Bayesian thinking is about changing your mind and updating how you understand the world. The data we observe is all that is real, so our beliefs ultimately need to shift until they align with the data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian Thinking and Everyday Reasoning</span>"
    ]
  },
  {
    "objectID": "01-everyday-reasoning.html#wrapping-up",
    "href": "01-everyday-reasoning.html#wrapping-up",
    "title": "\n1  Bayesian Thinking and Everyday Reasoning\n",
    "section": "\n1.5 Wrapping Up",
    "text": "1.5 Wrapping Up\n\n\n\n\n\n\nImportant\n\n\n\nYou should be far more concerned with data changing your beliefs \\(P(D \\mid H)\\) (Equation 1.3) than with ensuring data supports your beliefs, \\(P(H \\mid D)\\) (Equation 1.4).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian Thinking and Everyday Reasoning</span>"
    ]
  },
  {
    "objectID": "01-everyday-reasoning.html#exercises",
    "href": "01-everyday-reasoning.html#exercises",
    "title": "\n1  Bayesian Thinking and Everyday Reasoning\n",
    "section": "\n1.6 Exercises",
    "text": "1.6 Exercises\nTry answering the following questions to see how well you understand Bayesian reasoning. The solutions can be found at No Starch Press (PDF).\n\nExercise 1.1 Rewrite the following statements as equations using the mathematical notation you learned in this chapter:\n\nThe probability of rain is low: \\(P(rain) = low\\)\n\nThe probability of rain given that it is cloudy is high: \\(P(rain \\mid cloudy) = high\\)\n\nThe probability of you having an umbrella given it is raining is much greater than the probability of you having an umbrella in general: \\(P(\\text{I have umbrella} \\mid \\text{raining}) &gt;&gt; P(\\text{I have umbrella})\\)\n\n\n\n\n\nExercise 1.2 Organize the data you observe in the following scenario into a mathematical notation, using the techniques we’ve covered in this chapter. Then come up with a hypothesis to explain this data:\n\nYou come home from work and notice that your front door is open and the side window is broken. As you walk inside, you immediately notice that your laptop is missing.\n\n\\[\nP(\\text{door open, window broken, laptop missing} \\mid H_{hausbreaking})\n\\]\n\n\n\nExercise 1.3 The following scenario adds data to the previous one. Demonstrate how this new information changes your beliefs and come up with a second hypothesis to explain the data, using the notation you’ve learned in this chapter.\n\nA neighborhood child runs up to you and apologizes profusely for accidentally throwing a rock through your window. They claim that they saw the laptop and didn’t want it stolen so they opened the front door to grab it, and your laptop is safe at their house.\n\n\\[\nP(\\text{door open, window broken, laptop missing, child explains} \\mid H_{accident})\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Bayesian Thinking and Everyday Reasoning</span>"
    ]
  },
  {
    "objectID": "03-logic-uncertainty.html#combining-probabilities-with-and",
    "href": "03-logic-uncertainty.html#combining-probabilities-with-and",
    "title": "3  Logic of Uncertainty",
    "section": "",
    "text": "3.1.1 Solving a Combination of Two Probabilities\nSuppose we want to know the probability of getting a heads in a coin flip AND rolling a 6 on a die. We know that the probability of each of these events individually is:\n\\[P(heads) = \\frac{1}{2}, P(six) = \\frac{1}{6}\\]\nNow we want to know the probability of both of these things occurring, written as:\n\\[P(heads, six) = ?\\]\n\n\n3.1.2 Applying the Product Rule of Probability\n\n\nTheorem 3.1 (Product rule for combining probabilities) \\[P(A,B) = P(A) \\times P(B) \\tag{3.1}\\]\n\n\nIn our example:\n\\[P(heads,six) = \\frac{1}{2} \\times \\frac{1}{6} = \\frac{1}{12}\\]\n\n\n3.1.3 Example: Calculating the Probability of Being Late\nLet’s assume the local transit authority publishes data that tells us that 15 percent of the time the train is late, and 20 percent of the time the bus is late. Since you’ll be late only if both the bus and the train are late, we can use the product rule to solve this problem:\n\\[P(Late) = P(Late_{train}) \\times P(Late_{bus}) = 0.15 \\times 0.20 = 0.03\\] Even though there’s a pretty reasonable chance that either the bus or the train will be late, the probability that they will both be late is significantly less, at only 0.03. We can also say there is a 3 percent chance that both will be late. With this calculation done, you can be a little less stressed about being late.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Logic of Uncertainty</span>"
    ]
  },
  {
    "objectID": "03-logic-uncertainty.html#combining-probabilities-with-or",
    "href": "03-logic-uncertainty.html#combining-probabilities-with-or",
    "title": "3  Logic of Uncertainty",
    "section": "3.2 Combining Probabilities with OR",
    "text": "3.2 Combining Probabilities with OR\nThe probability of one event OR another event occurring is slightly more complicated because the events can either be mutually exclusive or not mutually exclusive. Events are mutually exclusive if one event happening implies the other possible events cannot happen.\n\n3.2.1 Calculating OR for Mutually Exclusive Events\nThe process of combining two events with OR feels logically intuitive. If you’re asked, “What is the probability of getting heads or tails on a coin toss?” you would say, “1.”, because we know that:\n\\[P(heads) = \\frac{1}{2}, P(tails) = \\frac{1}{2}\\]\nIntuitively, we might just add the probability of these events together. We know this works because heads and tails are the only possible outcomes, and the probability of all possible outcomes must equal 1.\nFrom this we can see that, as long as events are mutually exclusive, we can simply add up all of the probabilities of each possible event to get the probability of either event happening to calculate the probability of one event OR the other.\nThis addition rule applies only to combinations of mutually exclusive outcomes. In probabilistic terms, mutually exclusive means that:\n\\[P(A) \\operatorname{AND} P(B) = 0\\] To really understand combining probabilities with OR, we need to look at the case where events are not mutually exclusive.\n\n\n3.2.2 Using the Sum Rule for Non–Mutually Exclusive Events\nGiven that we know that \\(P(heads) = 1/2\\) and \\(P(six) = 1/6\\), it might initially seem plausible that the probability of either of these events is simply \\(4/6\\). It becomes obvious that this doesn’t work, however, when we consider the possibility of either flipping a heads (=1/2) or rolling a number less than \\(6\\). Because \\(P(\\text{less than six}) = 5/6\\), adding these probabilities together wit \\(1/2\\) gives us \\(8/6\\), which is greater than \\(1\\)! Since this violates the rule that probabilities must be between \\(0\\) and \\(1\\), we must have made a mistake.\nThe trouble is that flipping a heads and rolling a 6 are not mutually exclusive. As we know from earlier in the chapter, \\(P(heads, six) = 1/12\\). Because the probability of both events happening at the same time is not \\(0\\), we know they are, by definition, not mutually exclusive.\nThe reason that adding our probabilities doesn’t work for non–mutually exclusive events is that doing so doubles the counting of events where both things happen. To correct our probabilities, we must add up all of our probabilities and then subtract the probability of both events occurring.\n\n\nTheorem 3.2 (Sum Rule for Non–Mutually Exclusive Events) \\[P(A) \\operatorname{OR} P(B) = P(A) + P(B) – P(A,B) \\tag{3.2}\\]\n\n\nUsing our die roll and coin toss example, the probability of rolling a number less than 6 or flipping a heads is:\n\\[P(heads) \\operatorname{OR} P(six) = P(heads) + P(six) - P(heads, six) = \\frac{1}{2} + \\frac{1}{6} - \\frac{1}{12} = \\frac{7}{12}\\]\n\n\n3.2.3 Example: Calculating the Probability of Getting a Hefty Fine\nRemains empty: Doesn’t bring new knowledge.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Logic of Uncertainty</span>"
    ]
  },
  {
    "objectID": "03-logic-uncertainty.html#wrapping-up",
    "href": "03-logic-uncertainty.html#wrapping-up",
    "title": "3  Logic of Uncertainty",
    "section": "3.3 Wrapping Up",
    "text": "3.3 Wrapping Up\nWe’ve learned the logic of uncertainty by adding rules for combining probabilities with AND and OR.\n\nAND operator: Use the product rule as in Equation 3.1.\nOR mutually exclusive events: Add all probabilities together.\nOR not mutually exclusive events: Use the sum rule as in Equation 3.2.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Logic of Uncertainty</span>"
    ]
  },
  {
    "objectID": "03-logic-uncertainty.html#exercises",
    "href": "03-logic-uncertainty.html#exercises",
    "title": "3  Logic of Uncertainty",
    "section": "3.4 Exercises",
    "text": "3.4 Exercises\nTry answering the following questions to make sure you understand the rules of logic as they apply to probability. The solutions can be found at https://nostarch.com/learnbayes/.\n\nExercise 3.1 What is the probability of rolling a 20 three times in a row on a 20-sided die?\nSolution:\n\\[\n\\frac{1}{20} \\times \\frac{1}{20} \\times \\frac{1}{20} = \\frac{1}{8,000}\n\\]\n\n\nExercise 3.2 The weather report says there’s a 10 percent chance of rain tomorrow, and you forget your umbrella half the time you go out. What is the probability that you’ll be caught in the rain without an umbrella tomorrow?\nSolution: \\[\nP(rain) = 0.1 \\times P(\\text{no umbrella}) = 0.5 = 0.05\n\\]\n\n\nExercise 3.3 Raw eggs have a 1/20,000 probability of having salmonella. If you eat two raw eggs, what is the probability you ate a raw egg with salmonella?\nSolution: This are not mutually exclusive events as both raw eggs could have salmonella. So Equation 3.2 applies:\n\\[\n\\begin{align*}\n(\\frac{1}{20,000} + \\frac{1}{20,000}) - (\\frac{1}{20,000} \\times \\frac{1}{20,000}) = \\\\\n\\frac{2}{20,000} - \\frac{1}{400,000,000} = \\\\\n\\frac{40000}{400,000,000} - \\frac{1}{400,000,000} = \\\\\n\\frac{39,999}{400,000,000}\n\\end{align*}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nI had problems with the many zeros. My result was 39/400,000. I tried the calculation again with the help of R.\n\n\n\n\nListing 3.1: Exercise 3 of “Logic of Uncertainty” (Chapter 3)\n\n\n# disabling scientific notation\n# https://stackoverflow.com/a/27318351/7322615\noptions(scipen = 999) \n\n(1 / 2e4 + 1 / 2e4) - (1 / 2e4 * 1 / 2e4)\n\n\n\n\n[1] 0.0000999975\n\n\n\n\nListing 3.2: Exercise 3 of “Logic of Uncertainty” (Chapter 3)\n\n\n# to compare with:\n39999 / 4e8\n\n\n\n\n[1] 0.0000999975\n\n\n\n\nListing 3.3: Exercise 3 of “Logic of Uncertainty” (Chapter 3)\n\n\n# or witouth scientific notation\noptions(scipen = -999) \n39999 / 4e8\n\n\n\n\n[1] 9.99975e-05\n\n\n\n\n\n\nExercise 3.4 What is the probability of either flipping two heads in two coin tosses or rolling three 6s in three six-sided dice rolls?\nSolution:\n\\[\n\\begin{align*}\nP(heads) \\operatorname{AND} P(heads) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4} = P(h)\\\\\nP(six) \\operatorname{AND} P(six) \\operatorname{AND} P(six) = \\frac{1}{6} \\times \\frac{1}{6} \\times \\frac{1}{6} = \\frac{1}{216} = P(s)\\\\\nP(h) \\operatorname{OR} P(s) = (\\frac{1}{4} + \\frac{1}{216}) - (\\frac{1}{4} \\times \\frac{1}{216}) = \\\\\n\\frac{55}{216} - \\frac{1}{864} = \\frac{219}{864} = \\frac{73}{288}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nI did not calculate percent values. But this would be sensible to get the result into the probability scale from 0 to 100%. Then it would be more surprising to see that the probability of Exercise 3.4 was pretty high: a little more than 25%.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Logic of Uncertainty</span>"
    ]
  },
  {
    "objectID": "04-binomial-distribution.html#structure-of-a-binomial-distribution",
    "href": "04-binomial-distribution.html#structure-of-a-binomial-distribution",
    "title": "4  Creating a Binomial Probability Distribution",
    "section": "",
    "text": "Flipping two heads in three coin tosses\nBuying 1 million lottery tickets and winning at least once\nRolling fewer than three 20s in 10 rolls of a 20-sided die\n\n\n\nDefinition 4.1 (Parameter for the binomial distribution) All binomial distributions involve three parameters:\n\nk The number of outcomes we care about\nn The total number of trials\np The probability of the event happening\n\n\n\n\n\n\\(k = 2\\), the number of events we care about, in this case flipping a heads\n\\(n = 3\\), the number times the coin is flipped\n\\(p = 1/2\\), the probability of flipping a heads in a coin toss\n\n\n\nTheorem 4.1 (Shorthand notation of a binomial distribution) \\[B(k; n, p) \\tag{4.1}\\]\n\n\n\n\nB stands for binomial distribution\nk is separated from the other parameters by a semicolon. This is because when we are talking about a distribution of values, we usually care about all values of \\(k\\) for a fixed \\(n\\) and \\(p\\).\nB(k; n, p) denotes each value in the distribution\nB(n, p) denotes the whole distribution",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating a Binomial Probability Distribution</span>"
    ]
  },
  {
    "objectID": "04-binomial-distribution.html#understanding-and-abstracting-out-the-details-of-our-problem",
    "href": "04-binomial-distribution.html#understanding-and-abstracting-out-the-details-of-our-problem",
    "title": "4  Creating a Binomial Probability Distribution",
    "section": "4.2 Understanding and Abstracting Out the Details of Our Problem",
    "text": "4.2 Understanding and Abstracting Out the Details of Our Problem\nWe’ll continue with the example of calculating the probability of flipping two heads in three coin tosses. Since the number of possible outcomes is small, we can quickly figure out the results we care about with just pencil and paper.\n\\[HHT, HTH, THH\\] To start generalizing, we’ll break this problem down into smaller pieces we can solve right now, and reduce those pieces into manageable equations. As we build up the equations, we’ll put them together to create a generalized function for the binomial distribution.\n\n\nTheorem 4.2 (Permuation Example for the Binomial Distribution)  \n\nEach outcome we care about will have the same probability.\nEach outcome is just a permutation (GLOSSARY), or reordering, of the others\n\n\\[\n\\begin{align*}\nP({heads, heads, tails}) = P({heads, tails, heads}) = P({tails, heads, heads}) = \\\\\nP(\\text{Desired Outcome})\n\\end{align*}\n\\tag{4.2}\\]\nSee how to do this calculation with R in Section 4.7.1.\n\n\nThere are three outcomes, but only one of them can possibly happen and we don’t care which. And because it’s only possible for one outcome to occur, we know that these are mutually exclusive, denoted as:\n\\[P(\\{heads, heads, tails\\},\\{heads, tails, heads\\},\\{tails, heads, heads\\}) = 0\\] This makes using the sum rule of probability easy.\n\\[\n\\begin{align*}\nP(\\{heads, heads, tails\\} \\operatorname{OR} \\{heads, tails, heads\\} \\operatorname{OR} \\{tails, heads, heads\\}) = \\\\\nP(\\text{Desired Outcome}) + P(\\text{Desired Outcome}) + P(\\text{Desired Outcome}) = \\\\\n3 \\times P(Desired Outcome)\n\\end{align*}\n\\] The value “3” is specific to this problem and therefore not generalizable. We can fix this by simply replacing “3” with a variable called \\(N_{outcomes}\\).\n\n\nTheorem 4.3 (Solution with place holders) \\[B(k;n,p) = N_{outcomes} \\times P(\\text{Desired Outcome}) \\tag{4.3}\\]\n\n\nNow we have to figure out two subproblems:\n\nHow to count the number of outcomes we care about?\nHow to determine the probability for a single outcome?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating a Binomial Probability Distribution</span>"
    ]
  },
  {
    "objectID": "04-binomial-distribution.html#counting-our-outcomes-with-the-binomial-coefficient",
    "href": "04-binomial-distribution.html#counting-our-outcomes-with-the-binomial-coefficient",
    "title": "4  Creating a Binomial Probability Distribution",
    "section": "4.3 Counting Our Outcomes with the Binomial Coefficient",
    "text": "4.3 Counting Our Outcomes with the Binomial Coefficient\nFirst we need to figure out how many outcomes there are for a given k (the outcomes we care about) and n (the number of trials). For small numbers we can simply count. But it doesn’t take much for this to become too difficult to do by hand. The solution is combinatorics (GLOSSARY))`.\n\n4.3.1 Combinatorics: Advanced Counting with the Binomial Coefficient\nThere is a special operation in combinatorics, called the binomial coefficient, that represents counting the number of ways we can select k from n — that is, selecting the outcomes we care about from the total number of trials.\n\n\nTheorem 4.4 (Notation for the binomial coefficient) \\[\\binom{n}{k} \\tag{4.4}\\]\n\n\nWe read this as “n choose k”. In our example we would say “in three tosses choose two heads”:\n\\[\\binom{3}{2}\\]\n\n\nTheorem 4.5 (Definition of the binomial coefficient operation) \\[\\binom{n}{k} = \\frac{n!}{k! \\times (n-k)!} \\tag{4.5}\\]\n\n\nThe ! means factorial, which is the product of all the numbers up to and including the number before the ! symbol, so \\(5! = (5 × 4 × 3 × 2 × 1)\\).\nIn R we compute the binomial coefficient for the case of flipping two heads in three tosses with the following function call:\n\n\n\n\n\nListing 4.1: Compute the binomial coefficient for flipping two heads in three tosses\n\n\nchoose(3,2)\n\n\n\n\n[1] 3\n\n\n\nSee how to calculate the binomial coefficient with Base R in Section 4.7.2.\nWe can now replace \\(N_{Outcomes}\\) in Equation 4.3 with the binomial coefficient:\n\\[B(k;n,p) = \\binom{n}{k} \\times P(\\text{Desired Outcome})\\]\n\n\n4.3.2 Calculating the Probability of the Desired Outcome\nAll we have left to figure out is the \\(P(\\text{Desired Outcome})\\), which is the probability of any of the possible events we care about. So far we’ve been using \\(P(\\text{Desired Outcome})\\) as a variable to help organize our solution to this problem, but now we need to figure out exactly how to calculate this value.\nLet’s focus on a single case of our example of tow heads in three tosses: \\(HHT\\). Using the product rule and negation from the previous chapter, we can describe this problem as: \\[P(heads, heads, no heads) = P(heads, heads, 1-heads)\\] Now we can use the product rule from Equation 3.1:\n\\[\n\\begin{align*}\nP(heads, heads, 1-heads) = \\\\\nP(heads) \\times P(heads) \\times (1-P(heads)) = \\\\\nP(heads)^2 \\times (1-P(heads))^1\n\\end{align*}\n\\] You can see that the exponents for \\(P(heads)^2\\) and \\(1 – P(heads)^1\\) are just the number of heads and the number of not heads in our scenario. These equate to k, the number of outcomes we care about, and n – k, the number of trials minus the outcomes we care about. Puting all together:\n\\[\n\\binom{n}{k} \\times P(heads)^{k} \\times (1- P(heads))^{n-k}\n\\] Generalizing for any probability, not just heads, we replace \\(P(heads)\\) with just p. This gives us a general solution. Compare the following list with Definition 4.1.\n\nk, the number of outcomes we care about;\nn, the number of trials; and\np, the probability of the individual outcome.\n\n\n\nTheorem 4.6 (Probability Mass Function (PMF) for the Binomial Distribution) \\[\n\\binom{n}{k} \\times p^{k} \\times (1- p)^{n-k}\n\\tag{4.6}\\]\n\n\nEquation 4.6 is the basis of the binomial distribution. It is called a Probability Mass Function (GLOSSARY) (PMF). The mass part of the name comes from the fact that we can use it to calculate the amount of probability for any given k using a fixed n and p, so this is the mass of our probability.\nNow that we have this equation, we can solve any problem related to outcomes of a coin toss. For example, we could calculate the probability of flipping exactly 12 heads in 24 coin tosses:\n\\[\nB(12,24,\\frac{1}{2}) = \\binom{24}{12} \\times (\\frac{1}{2})^{12} \\times (1-\\frac{1}{2})^{24-12} = 0.1611803\n\\]\n\n\n\n\n\nListing 4.2: Calculate the probability of flipping exactly 12 heads in 24 coin tosses\n\n\nchoose(24,12) * (1 / 2)^(12) * (1 - 1/2)^(24 - 12)\n\n\n\n\n[1] 0.1611803\n\n\n\nThe calculation in Listing 4.2 is only valid for our concrete example.\nFor example, we can plug in all the possible values for k in 10 coin tosses into our PMF and visualize what the binomial distribution looks like for all possible values.\n\n\n\nFigure 4.1: Binomial Distribution for 10 Coin Flips\n\n\n\n\n\n\nSee my Figure 4.4 as a replication of Figure 4.1.\nWe can also look at the same distribution for the probability of getting a 6 when rolling a six-sided die 10 times, as shown in Figure 4.2.\n\n\n\nFigure 4.2: Binomial Distribution for 10 Coin Flips\n\n\n\n\n\n\nAgain I replicated Figure 4.2 with my Figure 4.6.\nBottom line of the discussion in this section: A probability distribution is a way of generalizing an entire class of problems.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating a Binomial Probability Distribution</span>"
    ]
  },
  {
    "objectID": "04-binomial-distribution.html#sec-gacha-games",
    "href": "04-binomial-distribution.html#sec-gacha-games",
    "title": "4  Creating a Binomial Probability Distribution",
    "section": "4.4 Example: Gacha Games",
    "text": "4.4 Example: Gacha Games\nThere is only one new content in this section: Instead of computing the probability of one event we are going to calculate the possibility of drawing at least one specific card from a pile of infinite numbers of cards where we know the probability of this card we are interested. The aim is to have a p of at last 50% with 100 trials and a probability of 0.720% for the card we are interested.\nAt first let us compute the probability for getting exactly one card we are interested with 100 draws form the pile. We know the probability to draw the featured card is 0.720%.\n\\[\\binom{100}{1} \\times 0.00720^{1} \\times (1- 0.00720)^{100-1}\\]\n\n\n\n\nListing 4.3: Draw exact one card that has p = 0.720%\n\n\ndbinom(1, 100, 0.00720)\n\n\n\n\n[1] 0.352085\n\n\nAnd now let’s compute the probability for at least one card we are interested.\nIn R, we can use the Binomial Cumulative Distribution Function pbinom() to automatically sum up all the values of the card we are interested in our PMF.\n\n\n\nFigure 4.3: How the pbinom() function works\n\n\n\n\n\n\nThe pbinom() function takes three required arguments and an optional fourth called lower.tail (which defaults to TRUE). When the fourth argument is TRUE, the first argument sums up all of the probabilities less than or equal to our argument. When lower.tail is set to FALSE, it sums up the probabilities strictly greater than the first argument. By setting the first argument to \\(0\\), we are looking at the probability of getting one or more the cards we are interested. We set lower.tail to FALSE because that means we want values greater than the first argument (by default, we get values less than the first argument).\n\n\n\n\nListing 4.4: Example Calculation with the pbinom() Function\n\n\npbinom(0, 100, 0.00720, lower.tail = FALSE)\n\n\n\n\n[1] 0.5145138\n\n\nVoilá! This is the same result as in @#fig-pbinom-function.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating a Binomial Probability Distribution</span>"
    ]
  },
  {
    "objectID": "04-binomial-distribution.html#wrapping-up",
    "href": "04-binomial-distribution.html#wrapping-up",
    "title": "4  Creating a Binomial Probability Distribution",
    "section": "4.5 Wrapping Up",
    "text": "4.5 Wrapping Up\nIn this chapter Will Kurt demonstrated how we can deduce intuitively the formula for the binomial coefficient (GLOSSARY).\n\n\n\n\n\n\nNote\n\n\n\nI have seen this monstrosity of expression for the binomial coefficient many times in different books and was always overwhelmed from its complexity. But this has changed now: Will Kurt succeeded to demystify the formula for me!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating a Binomial Probability Distribution</span>"
    ]
  },
  {
    "objectID": "04-binomial-distribution.html#exercises",
    "href": "04-binomial-distribution.html#exercises",
    "title": "4  Creating a Binomial Probability Distribution",
    "section": "4.6 Exercises",
    "text": "4.6 Exercises\nTry answering the following questions to make sure you’ve grasped binomial distributions fully. The solutions can be found at https://nostarch.com/learnbayes/.\n\n4.6.1 Exercise 4.1\n\nExercise 4.1 What are the parameters of the binomial distribution for the probability of rolling either a 1 or a 20 on a 20-sided die, if we roll the die 12 times?\n\nk = interested events: 1 and 20 = 2.\nn = number of trials = 12\np = probability for each trial = \\(\\frac{2}{12}\\)\n\n\n\n\n\nListing 4.5: Binomial distribution for the probability of rolling either a 1 or a 20 on a 20-sided die, if we roll the die 12 times\n\n\ndbinom(2, 12, 2/20)\n\n\n\n\n[1] 0.2301278\n\n\n\n\n\n\n4.6.2 Exercise 4.2\n\nExercise 4.2 There are four aces in a deck of 52 cards. If you pull a card, return the card, then reshuffle and pull a card again, how many ways can you pull just one ace in five pulls?\n\n\n\n\nListing 4.6: If you pull a card, return the card, then reshuffle and pull a card again, how many ways can you pull just one ace in five pulls?\n\n\ncombinat::combn(x = 1:5, m = 1, fun = tabulate, simplify = TRUE, nbins = 5)\n\n\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    0    0    0    0\n[2,]    0    1    0    0    0\n[3,]    0    0    1    0    0\n[4,]    0    0    0    1    0\n[5,]    0    0    0    0    1\n\n\n\n\nListing 4.7: If you pull a card, return the card, then reshuffle and pull a card again, how many ways can you pull just one ace in five pulls?\n\n\nchoose(5,1)\n\n\n\n\n[1] 5\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn contrast to the result in the solution manual I programmed the exercise with combinat::combn(). This correct solution pretends that I could use it for every possible arrangement. That is not true. For instance I could not manage to display the many ways of rolling two 6s in three rolls of a six-sided die.\n\n\n\n\n\n\n4.6.3 Exercise 4.3\n\nExercise 4.3 For the example in Exercise 4.2, what is the probability of pulling five aces in 10 pulls (remember the card is shuffled back in the deck when it is pulled)?\n\n\n\n\nListing 4.8: Probability of pulling five aces in 10 pulls with replacing\n\n\ndbinom(5, 10, 4 / 52) * 100\n\n\n\n\n[1] 0.04548553\n\n\n\n\n\n\n\n\nWarning\n\n\n\nOnly about 0.0455%. But this is different than the result in the solution manual with 1/32000 = 0.003125%. I don’t know why there is this difference.\n\n\n\n\n\n\n4.6.4 Exercise 4.4\n\nExercise 4.4 When you’re searching for a new job, it’s always helpful to have more than one offer on the table so you can use it in negotiations. If you have a 1/5 probability of receiving a job offer when you interview, and you interview with seven companies in a month, what is the probability you’ll have at least two competing offers by the end of that month?\n\n\n\n\nListing 4.9: Probability of at least 2 interviews each with 1/5 chance for a job offer having 7 interviews\n\n\npbinom(1, 7, 1/5, lower.tail = FALSE)\n\n\n\n\n[1] 0.4232832\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is chance of about 42% that you receive at least two job offers.\nIn my first trial I calculated wrongly the probability for exact 2 competing offers with dbinom(2, 7, 1/2) instead of at least 2 competing offers! Additionally I forgot to add lower.tail = FALSE. to calculate more than x: x = 1, more than 1 = 2, therefore the first parameter is 1 (and not 2 as could be thought wrongly). Not using lower.tail = FALSE means that the default value of lower.tail = TRUE computes the probability of \\(P[X &lt;= x]\\) (instead of \\(P[X &gt; x]\\)).\n\n\n\n\n\n\n4.6.5 Exercise 4.5\n\nExercise 4.5 You get a bunch of recruiter emails and find out you have 25 interviews lined up in the next month. Unfortunately, you know this will leave you exhausted, and the probability of getting an offer will drop to 1/10 if you’re tired. You really don’t want to go on this many interviews unless you are at least twice as likely to get at least two competing offers. Are you more likely to get at least two offers if you go for 25 interviews, or stick to just 7?\n\n\n\n\nListing 4.10: Probability of at least 2 interviews each with 1/10 chance for a job offer having 25 interviews\n\n\npbinom(1, 25, 1/10, lower.tail = FALSE)\n\n\n\n\n[1] 0.7287941\n\n\nWith a reduced probability per interview you raised your changes from 42,3% to 72,9%. But to get an job offer is not twice as likely so you stick with 7 interviews.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating a Binomial Probability Distribution</span>"
    ]
  },
  {
    "objectID": "04-binomial-distribution.html#experiments",
    "href": "04-binomial-distribution.html#experiments",
    "title": "4  Creating a Binomial Probability Distribution",
    "section": "4.7 Experiments",
    "text": "4.7 Experiments\n\n4.7.1 Permutation with R\nI want to get the same result as in Equation 4.2, but this time using R. What are the permutations of possible events by flipping two heads in three coin tosses?\nLet’s P(heads) = 1 and P(tails) = 0, then we can use combinat::combn(). The package {combinat} is not part of Base R, so you have to install it.\n\n\n\n\nListing 4.11: Permutations of possible events by flipping two heads in three coin tosses\n\n\ncombinat::combn(x = c(1,2,3), m = 2, fun = tabulate, simplify = TRUE, nbins = 3)\n\n\n\n\n     [,1] [,2] [,3]\n[1,]    1    1    0\n[2,]    1    0    1\n[3,]    0    1    1\n\n\nThe syntax is: combn(x, m, fun = NULL, simplify = TRUE, ...).\n\nx: vector source for combinations equivalent to the the number of events.\nm: number of elements we are interested in\nfun = function to be applied to each combination (may be null). I am using the base::tabulate() to take the integer-valued vector and counting the number of times each integer occurs in it.\nsimplify: logical, if FALSE, returns a list, otherwise returns vector or array.\n...: arguments for the used function. In our case nbins refers to the number of bin used by the tabulate() function.\n\nLet’s try another example to understand better the pattern of the combn() functions: What are the permutations of possible events by flipping a coin 5 times and getting three heads:\n\n\n\n\nListing 4.12: Permutations of possible events by flipping two heads in five coin tosses\n\n\ncombinat::combn(x = 1:5, m = 2, fun = tabulate, simplify = TRUE, nbins = 5)\n\n\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1    1    1    1    0    0    0    0    0     0\n[2,]    1    0    0    0    1    1    1    0    0     0\n[3,]    0    1    0    0    1    0    0    1    1     0\n[4,]    0    0    1    0    0    1    0    1    0     1\n[5,]    0    0    0    1    0    0    1    0    1     1\n\n\n\n\n\n\n\n\nTip\n\n\n\nThere is also a special package {dice} for the calculation of various dice-rolling events. We could for instance compute the probability “What is the probability of rolling two 6s in three rolls of a six-sided die?” directly with:\n\n\n\n\nListing 4.13: Compute the probability of rolling two 6s in three rolls of a six-sided die\n\n\ndice::getEventProb(nrolls = 3,\n                   ndicePerRoll = 2,\n                   nsidesPerDie = 6,\n                   eventList = list(6, 6))\n\n\n\n\n[1] 0.052512\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nActually I did not understand the many implications of computing combinations and / or permutations with different functions and different packages:\n\nutils::combn()\ncombinat::combn()\ngtools::combinations() and gtools::permutations()\npermute::permute(), permute::shuffle()\nbase::expand.grid()\ntidyr::expand(), tidyr::crossing(), tidyr::nesting(), tidyr::expand_grid()\n\nAs far as I understand from my study of the Statistical Rethinking book, these functions are an important topic to understand Bayesian Statistics. These types of functions are used for grid approximation and in Bayesian statistics to extract or draw samples from fit models (e.g., rethinking::extract.samples(), rethinking::extract.prior())\nI am sure I will need to come back to this issue and study available material more in detail! But at the moment I am stuck and will skip this subject.\n\n\n\n\n4.7.2 Compute binomial coefficient manually\nI want to replicate the Base R function chosse() with Equation 4.5. This involves to calculate factorials with the base::factorial() function.\n\\[\\binom{n}{k} = \\frac{n!}{k! \\times (n-k)!}\\]\n\n\n\n\nListing 4.14: Calculate the probability of flipping exactly 12 heads in 24 coin tosses\n\n\nmy_choose &lt;-  function(n, k) {\n    factorial(n) / (factorial(k) * factorial(n - k))\n}\n\nchoose(24, 12) == my_choose(24, 12)\n\n\n\n\n[1] TRUE\n\n\nCalculation of \\(\\binom{24}{12}\\):\n\nWith Base R function choose(24, 12) = 2.704156^{6}.\nWith my own function my_choose(24, 12) = 2.704156^{6}.\n\n\n\n4.7.3 Compute density of the binomial distribution\nTo generalize I write my own function for the density of the binomial distribution. I will use the same arguments names as in Definition 4.1.\n\n\n\n\nListing 4.15: Function for the density of the binomial distribution\n\n\nmy_dbinom &lt;- function(k, n, p) {\n    choose(n, k) * p^k * (1 - p)^(n - k)\n}\n\nmy_dbinom(12, 24, 0.5)\n\n\n\n\n[1] 0.1611803\n\n\nVoilá: It gives the same result as the manual calculation in Listing 4.2.\nI wonder if there is not a Base R function which does the same as my_dbinom(). I tried stats::dbinom() and it worked!\n\n\n\n\nListing 4.16: Base R dbinom() function calculates the density of the binomial distribution\n\n\ndbinom(12, 24, 0.5)\n\n\n\n\n[1] 0.1611803\n\n\nAgain the same result as in Listing 4.2 and Listing 4.15!\n\n\n4.7.4 Replication of the Binomial Distribution of 10 Coin Flips\nHere I am going to try to replicate Figure 4.1.\n\n\n\n\nListing 4.17: Replicate Figure 4.1: Binomial Distribution of 10 Coin Flips\n\n\nk_values &lt;- seq.int(from = 0, to = 10 , by = 1)\n\ndata.frame(x = k_values, \n           y = dbinom(k_values, 10, 0.5)) |&gt; \n    ggplot2::ggplot(ggplot2::aes(x = x, y = y)) +\n    ggplot2::geom_col()\n\n\n\n\n\n\n\nFigure 4.4: Replication of Figure 4.1: Binomial Distribution of 10 Coin Flips with {ggplot2}\n\n\n\n\n\n\n\n\nWriting Listing 4.17 I had troubles applying the correct geom. At first I used geom_bar() but this did not work until I learned that I have to add the option “stat = identity” or to use geom_col(). The difference is:\n\ngeom_bar() makes the height of the bar proportional to the number of cases in each group. It uses stat_count() by default, e.g. it counts the number of cases at each x position. If there aren’t cases but only the values then one has to add “stat = identity” to declare that ggplot2 should take the data “as-is”.\n\ngeom_col() instead takes the heights of the bars to represent values in the data. It uses stat_identity() and leaves the data “as-is” by default.\n\n\n\n\n\n\n\nTip\n\n\n\nDuring my research for writing Listing 4.17 I learned of the {tidydice} package. It simulates dice rolls and coin flips and can be used for teaching basic experiments in introductory statistics courses.\nWith {tidydice} we replicate Figure 4.1 with just 2 lines using the binom_coin() inside the plot_binom() function. In addition to the graphical distribution it print also the exact values on top of the bars.\n\n\n\n\nListing 4.18: Replicate Figure 4.1: Binomial Distribution of 10 Coin Flips {tidydice}\n\n\ntidydice::plot_binom(\n  tidydice::binom_coin(times = 10, sides = 2, success = 2),\n  title = \"Binomial distribution of 10 coin flips\"\n)\n\n\n\n\n\n\n\nFigure 4.5: Replication of Figure 4.1: Binomial Distribution of 10 Coin Flips with {tidydice}\n\n\n\n\n\n\n\n\n{tidydice} has many other functions related to coin and dice experiments.\n\n\n\n\n4.7.5 Replication of the Binomial Distribution of 10 Dice Rolls\nI will replicate Figure 4.2 with {ggplot2} and with {tidydice}:\n\n\n\n\nListing 4.19: Replicate Figure 4.2: Binomial Distribution of 10 Dice Rolls\n\n\nk_values &lt;- seq.int(from = 0, to = 10 , by = 1)\n\ndata.frame(x = k_values, \n           y = dbinom(k_values, 10, 1/6)) |&gt; \n    ggplot2::ggplot(ggplot2::aes(x = x, y = y)) +\n    ggplot2::geom_col()\n\n\n\n\n\n\n\nFigure 4.6: Replication of Figure 4.2: Binomial Distribution of 10 Dice Rolls with {ggplot2}\n\n\n\n\n\n\n\n\n\n\n\n\nListing 4.20: Replicate Figure 4.2: Binomial Distribution of 10 Dice Rolls {tidydice}\n\n\ntidydice::plot_binom(\n  tidydice::binom_dice(times = 10, sides = 6, success = 6),\n  title = \"Binomial distribution of 10 dice rolls\"\n)\n\n\n\n\n\n\n\nFigure 4.7: Replication of Figure 4.2: Binomial Distribution of 10 Dice Rolls with {tidydice}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Creating a Binomial Probability Distribution</span>"
    ]
  },
  {
    "objectID": "05-beta-distribution.html#a-strange-scenario-getting-the-data",
    "href": "05-beta-distribution.html#a-strange-scenario-getting-the-data",
    "title": "5  The Beta Distribution",
    "section": "",
    "text": "5.1.1 Distinguishing Probability, Statistics, and Inference\nIn all of the examples so far, outside of the first chapter, we’ve known the probability of all the possible events, or at least how much we’d be willing to bet on them. In real life we are almost never sure what the exact probability of any event is; instead, we just have observations and data.\nThis is commonly considered the division between probability and statistics. In probabilities, probability (GLOSSARY), we know exactly how probable all of our events are, and what we are concerned with is how likely certain observations are. For example, we might be told that there is 1/2 probability of getting heads in a fair coin toss and want to know the probability of getting exactly 7 heads in 20 coin tosses.\nIn statistics discipline, statistics (GLOSSARY), we would look at this problem backward: assuming you observe 7 heads in 20 coin tosses, what is the probability of getting heads in a single coin toss? In a sense, statistics is probability in reverse. The task of figuring out probabilities given data is called inferential statistics, inference (GLOSSARY), and it is the foundation of statistics.\n\n\n5.1.2 Collecting Data\nWe want to estimate the probability that the mysterious box will deliver two quarters, and to do that, we first need to see how frequently you win after a few more tries. We’ve got 14 wins and 27 losses.\nWithout doing any further analysis, you might intuitively want to update your guess that P(two quarters) = 1/2 to P(two quarters) = 14/41. But what about your original guess—does your new data mean it’s impossible that 1/2 is the real probability?\n\n\n5.1.3 Calculating the Probability of Probabilities\n\\[\n\\begin{align*}\nH_{1} \\space is \\space P(\\text{two coins}) = \\frac{1}{2} \\\\\nH_{2} \\space is \\space P(\\text{two coins}) = \\frac{14}{41}\n\\end{align*}\n\\] “How probable is what we observed if \\(H_{1}\\) were true versus if \\(H_{2}\\) were true?” We can easily calculate this using Equation 4.6 of the binomial distribution from Chapter 4.\n\n\n\n\nListing 5.1: Calculating the Probability of Probabilities with dbinom()\n\n\n(H1 &lt;- dbinom(14, 41, 1/2))\n\n\n\n\n[1] 0.01602537\n\n\n\n\nListing 5.2: Calculating the Probability of Probabilities with dbinom()\n\n\n(H2 &lt;- dbinom(14, 41, 14/41))\n\n\n\n\n[1] 0.1304709\n\n\nThis shows us that, given the data (observing 14 cases of getting two coins out of 41 trials), \\(H_{2}\\) is almost 10 times more probable than \\(H_{1}\\)! However, it also shows that neither hypothesis is impossible and that there are, of course, many other hypotheses we could make based on our data.\nIf we wanted to look for a pattern, we could pick every probability from 0.1 to 0.9, incrementing by 0.1; calculate the probability of the observed data in each distribution; and develop our hypothesis from that.\n\n\n\nFigure 5.1: Visualization of different hypotheses about the rate of getting two quarters\n\n\n\n\n\n\nEven with all these hypotheses, there’s no way we could cover every possible eventuality because we’re not working with a finite number of hypotheses. So let’s try to get more information by testing more distributions. If we repeat the last experiment, testing each possibility at certain increments starting with 0.01 and ending with 0.99, incrementing by only 0.01 would give us the results in Figure 5.2.\n\n\n\nFigure 5.2: We see a definite pattern emerging when we look at more hypotheses\n\n\n\n\n\n\nThis seems like valuable information; we can easily see where the probability is highest. Our goal, however, is to model our beliefs in all possible hypotheses (that is, the full probability distribution of our beliefs).\nThere are two problems:\n\nThere’s an infinite number of possible hypotheses, incrementing by smaller and smaller amounts doesn’t accurately represent the entire range of possibilities—we’re always missing an infinite amount. (In practice, this isn’t a huge problem.)\nThere are 11 dots above 0.1 right now, and we have an infinite number of points to add. This means that our probabilities don’t sum to 1!\n\nEven though there are infinitely many possibilities here, we still need them all to sum to 1. This is where the beta distribution comes in.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Beta Distribution</span>"
    ]
  },
  {
    "objectID": "05-beta-distribution.html#sec-beta-distribution",
    "href": "05-beta-distribution.html#sec-beta-distribution",
    "title": "5  The Beta Distribution",
    "section": "5.2 Beta Distribution",
    "text": "5.2 Beta Distribution\nUnlike the binomial distribution (GLOSSARY), which breaks up nicely into discrete values, the beta distribution (GLOSSARY) represents a continuous range of values, which allows us to represent our infinite number of possible hypotheses.\nWe define the beta distribution with a probability density function PDF (GLOSSARY), which is very similar to the probability mass function we use in the binomial distribution, but is defined for continuous values.\n\nTheorem 5.1 (Formula for the PDF of the beta function) \\[\nBeta(p; \\alpha,\\beta) = \\frac{p^{\\alpha - 1} \\times (1 - p)^{\\beta - 1}}{beta(\\alpha, \\beta)}\n\\tag{5.1}\\]\n\n\n5.2.1 Breaking Down the Probability Density Function\np: Represents the probability of an event. This corresponds to our different hypotheses for the possible probabilities for our black box.\n$\\alpha$: Represents how many times we observe an event we care about, such as getting two quarters from the box.\n$\\beta$: Represents how many times the event we care about didn’t happen. For our example, this is the number of times that the black box ate the quarter.\n$\\alpha + \\beta$: The total number of trials. This is different than the binomial distribution, where we have k observations we’re interested in and a finite number of n total trials.\nThe top part of the PDF (GLOSSARY) function should look pretty familiar because it’s almost the same as the binomial distribution’s PMF (GLOSSARY) in Equation 4.6.\nDifferences between the PMF of the binomial distribution and the PDF of the beta distribution:\n\nIn the PDF, rather than \\(p^{k} \\times (1- p)^{n-k}\\), we have \\(p^{\\alpha - 1} \\times (1 - p)^{\\beta - 1}\\) where we subtract 1 from the exponent terms.\nWe also have another function in the denominator of our equation: the beta function (note the lowercase) for which the beta distribution is named. We subtract 1 from the exponent and use the beta function to normalize our values—this is the part that ensures our distribution sums to 1. The beta function is the integral from 0 to 1 of \\(p^{\\alpha - 1} \\times (1 - p)^{\\beta - 1}\\). (A discussion of how subtracting 1 from the exponents and dividing by the beta functions normalizes our values is beyond the scope of this chapter.)\n\nWhat we get in the end is a function that describes the probability of each possible hypothesis for our true belief in the probability of getting two heads from the box, given that we have observed \\(\\alpha\\) examples of one outcome and \\(\\beta\\) examples of another. Remember that we arrived at the beta distribution by comparing how well different binomial distributions, each with its own probability \\(p\\), described our data. In other words, the beta distribution represents how well all possible binomial distributions describe the data observed.\n\n\n5.2.2 Applying the Probability Density Function to Our Problem\nWhen we plug in our values for our black box data and visualize the beta distribution, shown in Figure 5.3, we see that it looks like a smooth version of the plot in Figure 5.2.\n\n\n\nFigure 5.3: Visualizing the beta distribution for our data collected about the black box\n\n\n\n\n\n\nWhile we can see the distribution of our beliefs by looking at a plot, we’d still like to be able to quantify exactly how strongly we believe that “the probability that the true rate at which the box returns two quarters is less than 0.5.”\n\n\n5.2.3 Quantifying Continuous Distributions with Integration\nThe beta distribution is fundamentally different from the binomial distribution in that with the latter, we are looking at the distribution of \\(k\\), the number of outcomes we care about, which is always something we can count. For the beta distribution, however, we are looking at the distribution of \\(p\\), for which we have an infinite number of possible values.\nWe know that the fundamental rule of probability is that the sum of all our values must be 1, but each of our individual values is infinitely small, meaning the probability of any specific value is in practice 0.\n\n\n\n\n\n\nNote\n\n\n\nThe zero probability of an event in a continuous distribution does not mean that this event never could happen. Zero probability only means that the events gets the probability measure of zero.\nOur intuition from discrete probability is that if an outcome has zero probability, then the outcome is impossible. With continuous random variables (or more generally, an infinite number of possible outcomes) that intuition is flawed. (StackExchange)\n\n\nFor example, even if we divided a 1-pound bar of chocolate into infinitely many pieces, we can still add up the weight of the pieces in one half of the chocolate bar. Similarly, when talking about probability in continuous distributions, we can sum up ranges of values. But if every specific value is 0, then isn’t the sum just 0 as well?\nThis is where calculus comes in: in calculus, there’s a special way of summing up infinitely small values called the integral.\nIf we want to know whether the probability that the box will return a coin is less than 0.5 (that is, the value is somewhere between 0 and 0.5), we can sum it up like this:\n\\[\\int_{0}^{0.5} \\frac{p^{14 - 1} \\times (1 - p)^{27 - 1}}{beta(14, 27)} \\tag{5.2}\\]\nR includes a function called dbeta() that is the PDF for the beta distribution. This function takes three arguments, corresponding to \\(p\\), \\(\\alpha\\), and \\(\\beta\\). We use this together with R’s integrate() function to perform this integration automatically. Here we calculate the probability that the chance of getting two coins from the box is less than or equal to 0.5, given the data:\n\n\n\n\nListing 5.3: Probability of getting two coins from the box is less than or equal to 0.5, given the data\n\n\nintegrate(function(p) dbeta(p,14,27),0,0.5)\n\n\n\n\n0.9807613 with absolute error &lt; 5.9e-06\n\n\nThe “absolute error” message appears because computers can’t perfectly calculate integrals so there is always some error, though usually it is far too small for us to worry about. This result from R tells us that there is a 0.98 probability that, given our evidence, the true probability of getting two coins out of the black box is less than 0.5. This means it would not be good idea to put any more quarters in the box, since you very likely won’t break even.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Beta Distribution</span>"
    ]
  },
  {
    "objectID": "05-beta-distribution.html#reverse-engineering-the-gacha-game",
    "href": "05-beta-distribution.html#reverse-engineering-the-gacha-game",
    "title": "5  The Beta Distribution",
    "section": "5.3 Reverse-Engineering the Gacha Game",
    "text": "5.3 Reverse-Engineering the Gacha Game\nIn real-life situations, we almost never know the true probabilities for events. That’s why the beta distribution is one of our most powerful tools for understanding our data.\nIn Section 4.4 of Chapter 4 , we knew the probability of the card we wanted to pull. In reality, the game developers are very unlikely to give players this information, for many reasons (such as not wanting players to calculate how unlikely they are to get the card they want).\nThis time we don’t know the rates for the card, but we really want that card—and more than one if possible. We spend a ridiculous amount of money and find that from 1,200 cards pulled, we received only 5 cards we’re interested in. Our friend is thinking of spending money on the game but only wants to do it if there is a better than 0.7 probability that the chance of pulling the card is greater than 0.005.\nOur data tells us that of 1,200 cards pulled, only 5 were cards we are interested in, so we can visualize this as Beta(5,1195), shown in Figure 5.4 (remember that the total cards pulled is \\(\\alpha + \\beta\\)).\n\n\n\nFigure 5.4: The beta distribution for getting the card we are interested in, given our data\n\n\n\n\n\n\nFrom our visualization we can see that nearly all the probability density is below 0.01. We need to know exactly how much is above 0.005, the value that our friend cares about. We can solve this by integrating over the beta distribution in R:\n\n\n\n\nListing 5.4: Probability that the rate of pulling a card we are interested is 0.005 or greater\n\n\nintegrate(function(x) dbeta(x,5,1195),0.005,1)\n\n\n\n\n0.2850559 with absolute error &lt; 1e-04\n\n\nThis tells us the probability that the rate of pulling a card we are interested is 0.005 or greater, given the evidence we have observed, is only 0.29. Our friend will pull for this card only if the probability is around 0.7 or greater, so based on the evidence from our data collection, our friend should not try his luck.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Beta Distribution</span>"
    ]
  },
  {
    "objectID": "05-beta-distribution.html#wrapping-up",
    "href": "05-beta-distribution.html#wrapping-up",
    "title": "5  The Beta Distribution",
    "section": "5.4 Wrapping Up",
    "text": "5.4 Wrapping Up\nWe learned about the beta distribution, which is closely related to the binomial distribution but behaves quite differently. The major difference between the beta distribution and the binomial distribution is that the beta distribution is a continuous probability distribution. Because there are an infinite number of values in the distribution, we cannot sum results the same way we do in a discrete probability distribution. Instead, we need to use calculus to sum ranges of values. Fortunately, we can use R instead of solving tricky integrals by hand.\nWe built up to the beta distribution by observing how well an increasing number of possible binomial distributions explained our data. The beta distribution allows us to represent how strongly we believe in all possible probabilities for the data we observed. This enables us to perform statistical inference on observed data by determining which probabilities we might assign to an event and how strongly we believe in each one: a probability of probabilities.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Beta Distribution</span>"
    ]
  },
  {
    "objectID": "05-beta-distribution.html#exercises",
    "href": "05-beta-distribution.html#exercises",
    "title": "5  The Beta Distribution",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises\nTry answering the following questions to make sure you understand how we can use the Beta distribution to estimate probabilities. The solutions can be found at https://nostarch.com/learnbayes/.\n\n5.5.1 Exercise 5-1\nYou want to use the beta distribution (GLOSSARY) to determine whether or not a coin you have is a fair coin — meaning that the coin gives you heads and tails equally. You flip the coin 10 times and get 4 heads and 6 tails. Using the beta distribution, what is the probability that the coin will land on heads more than 60 percent of the time?\n\n\n\n\nListing 5.5: Probability that a coin will land on heads more than 60% given 4 heads in 10 tosses\n\n\nintegrate(function(p) dbeta(p, 4, 6), 0.6, 1)\n\n\n\n\n0.09935258 with absolute error &lt; 1.1e-15\n\n\n\n\n5.5.2 Exercise 5-2\nYou flip the coin 10 more times and now have 9 heads and 11 tails total. What is the probability that the coin is fair, using our definition of fair, give or take 5 percent?\n\n\n\n\nListing 5.6: Probability that a coin is fair within a 5% range given 9 heads in 20 tosses\n\n\nintegrate(function(p) dbeta(p, 9, 11), 0.45, 0.55)\n\n\n\n\n0.30988 with absolute error &lt; 3.4e-15\n\n\n\n\n5.5.3 Exercise 5-3\nData is the best way to become more confident in your assertions. You flip the coin 200 more times and end up with 109 heads and 111 tails. Now what is the probability that the coin is fair, give or take 5 percent?\n\n\n\n\nListing 5.7: Probability that a coin is fair within a 5% range given 109 heads in 220 tosses\n\n\nintegrate(function(p) dbeta(p, 109, 111), 0.45, 0.55)\n\n\n\n\n0.8589371 with absolute error &lt; 9.5e-15",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Beta Distribution</span>"
    ]
  },
  {
    "objectID": "05-beta-distribution.html#experiments",
    "href": "05-beta-distribution.html#experiments",
    "title": "5  The Beta Distribution",
    "section": "5.6 Experiments",
    "text": "5.6 Experiments\n\n5.6.1 Replicating Figure 5.1\nTo replicate Figure 5.1 we need to pick every probability from 0.1 to 0.9, incrementing by 0.1; and then to calculate the probability of the observed data (14 cases of getting two coins out of 41 trials) in each distribution:\n\n\n\n\nListing 5.8: Replication of Figure 5.1: Visualization of different hypotheses about the rate of getting two quarters from the black box\n\n\ntibble::tibble(x = seq(from = 0.1, to = 0.9, by = 0.1),\n                    y = dbinom(14, 41, x)) |&gt; \n\nggplot2::ggplot(ggplot2::aes(x = x, y = y)) +\n    ggplot2::geom_point() +\n    ggplot2::theme_bw() +\n    ggplot2::labs(\n        title = \"Probability of different values for p given observation\",\n        x = \"p\",\n        y = \"Probability\"\n    )\n\n\n\n\n\n\n\nFigure 5.5: Visualization of different hypotheses about the rate of getting two quarters from the black box\n\n\n\n\n\n\n\n\n\n\n5.6.2 Replicating Figure 5.2\nRepeating Section 5.6.1, we want to display each possibility at smaller increments starting with 0.01 and ending with 0.99, incrementing by only 0.01:\n\n\n\n\nListing 5.9: Replication of Figure 5.2: We see a definite pattern emerging when we look at more hypotheses\n\n\ntibble::tibble(x = seq(from = 0.01, to = 0.99, by = 0.01),\n                    y = dbinom(14, 41, x)) |&gt; \n\nggplot2::ggplot(ggplot2::aes(x = x, y = y)) +\n    ggplot2::geom_point() +\n    ggplot2::theme_bw() +\n    ggplot2::labs(\n        title = \"Probability of different values for p given observation\",\n        x = \"p\",\n        y = \"Probability\"\n    )\n\n\n\n\n\n\n\nFigure 5.6: A definite pattern emerging when we look at more hypotheses\n\n\n\n\n\n\n\n\n\n\n5.6.3 Replicating Figure 5.3\nOur data: We’ve got 14 successes (two coins) with 41 trials.\n\n\n\n\nListing 5.10: Replication of Figure 5.3: Visualizing the beta distribution for our data collected about the black box\n\n\ntibble::tibble(x = seq(from = 0, to = 1, by = 0.01),\n                    y = dbeta(x, 14, 27)) |&gt; \n\nggplot2::ggplot(ggplot2::aes(x = x, y = y)) +\n    ggplot2::geom_line() +\n    ggplot2::theme_bw() +\n    ggplot2::labs(\n        title = \"Distribution for Beta(14, 27)\",\n        x = \"p\",\n        y = \"Probability\"\n    )\n\n\n\n\n\n\n\nFigure 5.7: Visualizing the beta distribution for our data collected about the black box\n\n\n\n\n\n\n\n\n\n\n5.6.4 Replicating Figure 5.4\nOur data tells us that of 1,200 cards pulled, there were only 5 cards we are interested in. Our friend is thinking of spending money on the game but only wants to do it if there is a better than 0.7 probability that the chance of pulling a Bradley Efron (the card we are interested) is greater than 0.005.\n\n\n\n\nListing 5.11: Replication of Figure 5.4: The beta distribution for getting the card we are interested in, given our data\n\n\ntibble::tibble(x = seq(from = 0, to = 1, length = 1000),\n              y = dbeta(x,5,1195)) |&gt; \n\nggplot2::ggplot(ggplot2::aes(x = x, y = y)) +\n    ggplot2::geom_line() +\n    ggplot2::theme_bw() +\n    ggplot2::labs(\n        title = \"Pulling a Bradley Efron Card, Beta(5, 1195)\",\n        x = \"p\",\n        y = \"Probability\"\n)\n\n\n\n\n\n\n\nFigure 5.8: The beta distribution form 0 to 1 for getting the card we are interested in, given our data\n\n\n\n\n\n\n\n\nThis was my first try. At first I thought that my Listing 5.11 is wrong as Figure 5.4 has a very different appearance. But then I noticed that my graphics displays value from 0 to 1 whereas Figure 5.4 visualizes only values between 0 and 0.01!\nIn Listing 5.12 I changed the visualization just showing values from 0 to 0.01.\n\n\n\n\nListing 5.12: Replication of Figure 5.4: The beta distribution for getting the card we are interested in, given our data\n\n\ntibble::tibble(x = seq(from = 0, to = 0.01, length = 1000),\n              y = dbeta(x,5,1195)) |&gt; \n\nggplot2::ggplot(ggplot2::aes(x = x, y = y)) +\n    ggplot2::geom_line() +\n    ggplot2::theme_bw() +\n    ggplot2::labs(\n        title = \"Pulling a Bradley Efron Card, Beta(5, 1195)\",\n        x = \"p\",\n        y = \"Probability\"\n)\n\n\n\n\n\n\nThe beta distribution form 0 to 0.01 for getting the card we are interested in, given our data",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Beta Distribution</span>"
    ]
  },
  {
    "objectID": "index.html#sec-book-motivation",
    "href": "index.html#sec-book-motivation",
    "title": "Bayesian Statistics the Fun Way. Een notitieboek",
    "section": "About My Motivation for this Quarto Book",
    "text": "About My Motivation for this Quarto Book\nI am not an expert in statistics. During my study in sociology back in 1970er I had only rudimentary learned about frequentist statistics (GLOSSARY). There weren’t computer only via time-sharing and keypunching for card-to-tape converter available. This was painstaking and not motivating because it had not much practical values. 10 years later I worked sometimes with SPSS but used — as many of my colleagues — the “statistics all” option without much understanding. It was no fun and so I dedicated most of my time to theoretical work and later sometimes to qualitative social research via grounded theory.\nOnly when I heard 2015 about R and began to exerpiment with it, I started my self-directed education in statistics again. Still I was not so intrigued by Null Hypothesis Statistical Testing (NHST) but more from Data Science. I noticed the problems with p-values and was therefore fascinated with Introduction to the New Statistics by Geoff Cumming & Robert Calin-Jageman (cumming2017?). Instead of concentrating on p-values the book teaches the importance of confidence interval,confidence intervals (GLOSSARY), (CIs). But at that time I came across some discussion about Bayesian statistics (GLOSSARY. I read The Theory That Would Not Die: How Bayes’ Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant From Two Centuries of Controversy by Sharon Bertsch McGrayne (mcgrayne2011?) but didn’t understand at that time how Bayesian statistics works.\nThe real game changer for me then was Statistical Rethinking by Richard McElreath (mcelreath2020?). I immediately started my first Quarto book with notes about the book and the companion bookdown website Statistical rethinking with brms, ggplot2, and the tidyverse: Second edition by A Solomon Kurz. But unfortunately after four chapters I noticed that I am lacking basic knowledge and looked around for other books on Bayesian statistics that are easier to digest for me. I tried A Student’s Guide to Bayesian Statistics by Ben Lambert (lambert2018?), but I stopped reading it after chapter eight. I has to much theory for me and I was missing the opportunity trying my own hands out with R.\nSo finally I came around to Will Kurt’s book. I understand that the modern simulation methods like MCMC are not in the focus of the book and also the usage of R code is distributed very sparsely in the book. But I learned (and understood) many Bayesian concepts and could all the graphics in the book — where the R code is missing — replicate with {ggplot2} (ggplot2?) in tidyverse (tidyverse?) style. The book was at the time (September/October 2023) a perfect match for my rudimentary knowledge. I have now more trust to continue with Statistical Rethinking or — as a possible alternative — to start with a new Quarto book on Doing Bayesian Data Analysis: A Tutorial With R, JAGS, and Stan (kruschke2014?) which I have already read and (I believe) mostly understood.\nAnother motivation to write a Quarto book was to learn how to use Quarto. I already wrote some books with bookdown (bookdown?) but Quarto was relatively new for me. This book was therefore a good occasion to learn and experiment with the functionality of Quarto.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#content-and-goals-of-this-book",
    "href": "index.html#content-and-goals-of-this-book",
    "title": "Bayesian Statistics the Fun Way. Een notitieboek",
    "section": "Content and Goals of this Book",
    "text": "Content and Goals of this Book\nThis book collects personal notes during reading of Bayesian Statistics the Fun Way by Will Kurt. Additionally I am using C: Answers to Exercises\nEach chapter of the book has three main parts:\nShort summaries of the book content\nThis summarizes my own highlights but gives also the necessary information for my section about “Experiments”. Mostly I quote the text but without page references. Often I made minor editing (e.g., shorting the text) or put sometimes the content in my own wording. As I follow the text section by section reader can easily find the quoted passage.\nAlmost all of my text of this first part of the Quarto book are not mine, but is coming from the resources mentioned above. There are two exceptions:\n\nWhenever necessary I include personal notes already in this part of the chapter. Most of the time it is combined with a cross reference to the third main part of the book: my experiments.\nSometimes there is also base R Code by the author provided. Whenever possible I convert this base R code by the author in {tidyverse} code.\nI copied all figures into the text as a template for my own replication.\n\n\n\n\n\n\n\nImportant\n\n\n\nAlthough I have quoted many passages from the original as highlights for me to remember, this Quarto ebook is not meant to stand alone. My summaries are driven by my personal interests and my huge gaps in my statistical knowledge and especially in Bayesian statistics. My notes are therefore in no way a substitute for Will Kurt’s book Bayesian Statistics the Fun Way: Understanding Statistics and Probability With Star Wars, LEGO, and Rubber Ducks by Will Kurt (kurt2019?). Please buy the book so that you can embed my notes appropriately in the general argumentation of the author.\n\n\nExercises\nAgain I quote the full question text and then try my own solution. If my solution is not correct or I cant find a solution I will note this in a personal callout and will correct the solution. If my solution is correct but with a different method I will also reflect on the result, but will not adapt or change my solution.\nExperiments\nHere I am trying to make my own exercises. Most of the examples are replications of the book figures because the author has not included the R code. Additionally I cross referenced to the figure from the first part, the author section. If you hover over the cross reference link you will get the authors figure overlaid and can compare it with my replication.\nWriting my own R chunks I am using the tidy approach with the collection of the {tidyverse} packages, especially with {ggplot2} for the figures. But instead of using the library() command I always mention the used packages explicitly. Whenever I used another packages I called the function with the package name in front with the syntax &lt;package name&gt;::&lt;function name&gt;(), like ggplot2::gplot(). This is tedious work but it helps me to learn & remember which command “belongs” to which package. Only exception is the {patchwork} package, as I do not know how to call commands like p1 + p2 with the package name.\nIn graphics I use not only caption for figures but also captions for the R code. There is no easy standardized way to use code listings with captions and to evaluate the code at the same time, but I found a workaround with the following structure of code options in the code chunk:\n#| label: fig-name\n#| fig-cap: \"fig-title\"\n#| attr-source: '#lst-fig-name lst-cap=\"list-title\"'\nThis code generates a warning because at the moment the = symbol is not allowed in RStudio when running a R chunks interactively in a Quarto document. See issue 13326. Although I got a warning the code does what I want: It gives the figure and the code listing a heading and evaluates the code at the same time.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#glossary",
    "href": "index.html#glossary",
    "title": "Bayesian Statistics the Fun Way. Een notitieboek",
    "section": "Glossary",
    "text": "Glossary\nI started a glossary using the {glossary} package by Lisa DeBruine (glossary?). A glossary entry is visualized with a double underline. Every chapter has a section where the text of all glossary entries of the chapter are displayed. When you hover with the mouse over the link it opens a pop-up window with the glossary text. You can see example in the section about my motivation for this ebook (See About My Motivation for this Quarto Book).\nI am using this glossary for all my other Quarto books but it is till work in progress. Please keep in mind that I collected the definition at various places and be attentive because there is no guarantee that the entry is appropriate and correct. I have added the sources where I got the content for the glossary entry.\n\n\n\n\n\n\nNote\n\n\n\nLoad glossary\n\nInstall the package glossary https://debruine.github.io/glossary/\n\nlibrary(glossary)\nIf you want to see Baumgartner’s glossary:\n\nfork his repository https://github.com/petzi53/glossary-pb\nDownload the glossary-pb repository to your computer https://github.com/petzi53/glossary-pb/blob/master/glossary.yml)\nStore the file on your hard disk and change the following path to your path\n\nglossary::glossary_path(\"../glossary.yml\")\n\n\n::: ## Session Info\nEvery chapter ends with a session information printed with the sessionInfo() function.\n\n\n\n\n\n\nWarning\n\n\n\nI wrote this book as a text for others to read because that forces me to be become explicit and explain all my learning outcomes more carefully. Please keep in mind that this text is not written by an expert but by a learner. In spite of replicating most of the content it may contain many mistakes. All these misapprehensions and errors are my responsibility.\nIn any case I am the only responsible person for this text, especially if I have used code from the resources wrongly or misunderstood a quoted text passage.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#session-info",
    "href": "index.html#session-info",
    "title": "Bayesian Statistics the Fun Way. Een notitieboek",
    "section": "Session Info",
    "text": "Session Info\nEvery chapter ends with a session information printed with the sessionInfo() function.\n\n\n\n\n\n\nWarning\n\n\n\nI wrote this book as a text for others to read because that forces me to be become explicit and explain all my learning outcomes more carefully. Please keep in mind that this text is not written by an expert but by a learner. In spite of replicating most of the content it may contain many mistakes. All these misapprehensions and errors are my responsibility.\nIn any case I am the only responsible person for this text, especially if I have used code from the resources wrongly or misunderstood a quoted text passage.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#install-the-package-glossary",
    "href": "index.html#install-the-package-glossary",
    "title": "Bayesian Statistics the Fun Way. Een notitieboek",
    "section": "1. Install the package glossary",
    "text": "1. Install the package glossary",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#httpsdebruine.github.ioglossary",
    "href": "index.html#httpsdebruine.github.ioglossary",
    "title": "Bayesian Statistics the Fun Way. Een notitieboek",
    "section": "https://debruine.github.io/glossary/",
    "text": "https://debruine.github.io/glossary/\nlibrary(glossary)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#if-you-want-to-see-baumgartners-glossary",
    "href": "index.html#if-you-want-to-see-baumgartners-glossary",
    "title": "Bayesian Statistics the Fun Way. Een notitieboek",
    "section": "If you want to see Baumgartner’s glossary:",
    "text": "If you want to see Baumgartner’s glossary:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#fork-his-repository",
    "href": "index.html#fork-his-repository",
    "title": "Bayesian Statistics the Fun Way. Een notitieboek",
    "section": "1. fork his repository",
    "text": "1. fork his repository",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#httpsgithub.competzi53glossary-pb",
    "href": "index.html#httpsgithub.competzi53glossary-pb",
    "title": "Bayesian Statistics the Fun Way. Een notitieboek",
    "section": "https://github.com/petzi53/glossary-pb",
    "text": "https://github.com/petzi53/glossary-pb",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#download-the-glossary-pb-repository-to-your-computer",
    "href": "index.html#download-the-glossary-pb-repository-to-your-computer",
    "title": "Bayesian Statistics the Fun Way. Een notitieboek",
    "section": "2. Download the glossary-pb repository to your computer",
    "text": "2. Download the glossary-pb repository to your computer",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#httpsgithub.competzi53glossary-pbblobmasterglossary.yml",
    "href": "index.html#httpsgithub.competzi53glossary-pbblobmasterglossary.yml",
    "title": "Bayesian Statistics the Fun Way. Een notitieboek",
    "section": "https://github.com/petzi53/glossary-pb/blob/master/glossary.yml)",
    "text": "https://github.com/petzi53/glossary-pb/blob/master/glossary.yml)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#store-the-file-on-your-hard-disk",
    "href": "index.html#store-the-file-on-your-hard-disk",
    "title": "Bayesian Statistics the Fun Way. Een notitieboek",
    "section": "3. Store the file on your hard disk",
    "text": "3. Store the file on your hard disk",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#and-change-the-following-path-to-your-path",
    "href": "index.html#and-change-the-following-path-to-your-path",
    "title": "Bayesian Statistics the Fun Way. Een notitieboek",
    "section": "and change the following path to your path",
    "text": "and change the following path to your path\nglossary::glossary_path(“../glossary.yml”)",
    "crumbs": [
      "Preface"
    ]
  }
]